+++
title = "Guided Policy Search"
date = 2020-03-24T14:04:44-04:00
draft = false
categories = []
summary = "TODO"
+++

### Source
[Levine, Sergey, and Vladlen Koltun. "Guided policy search." International Conference on Machine Learning. 2013.](http://proceedings.mlr.press/v28/levine13.pdf)

### Overview
* Show how differential dynamic programming (DDP) can be used to supplement the sample set with off-policy guiding samples that guide the policy search to regions of high reward.
* From [Esteban, Rozo 2018]:
    * (GPS) algorithms are data-efficient methods for learning NN policies because they transform the policy search problem into supervised learning, where the training data is generated by a computational teacher that produces data that is best suited for training the final policy.
    * The possibility to use robust algorithms and simple local policies with few parameters has made GPS a popular framework to learn complex policies. The simple local policies are employed as computational teachers that generate guiding distributions for a nonlinear global policy.
    * Instead of optimizing the parameters directly from the expected cost, GPS methods transform the policy search problem into supervised learning, where the training set is generated by a computational teacher, optimized by either simple trajectory-centric RL algorithms or complex trajectory optimization methods.
