+++
title = "Guided Policy Search"
date = 2020-03-24T14:04:44-04:00
draft = false
categories = []
summary = "TODO"
+++

### Source
[Levine, Sergey, and Vladlen Koltun. "Guided policy search." International Conference on Machine Learning. 2013.](http://proceedings.mlr.press/v28/levine13.pdf)

### Overview
* Show how differential dynamic programming (DDP) can be used to supplement the sample set with off-policy guiding samples that guide the policy search to regions of high reward.
* From [Esteban, Rozo 2018]:
    * (GPS) algorithms aredata-efﬁcient methods for learning NN policies because they1Department of Advanced Robotics, Istituto Italiano di Tecnologia, ViaMorego 30, 16163 Genova, Italy name.surname@iit.it2DIBRIS, Universit`a di Genova, Via Opera Pia 13, 16145, Italytransform the policy search problem into supervised learning,where the training data is generated by a computationalteacher that produces data that is best suited for trainingthe ﬁnal policy.
    * The possibility to use robust algorithms and simple localpolicies with few parameters has made GPS a popular frame-work to learn complex policies. The simple local policies areemployed as computational teachers that generate guidingdistributions for a nonlinear global policy.
    * Instead of optimizing the parameters directly from theexpected cost, GPS methods transform the policy searchproblem into supervised learning, where the training setis generated by a computational teacher, optimized by ei-ther simple trajectory-centric RL algorithms or complextrajectory optimization methods.
